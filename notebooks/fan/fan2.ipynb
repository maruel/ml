{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018395d-4d4c-4999-a557-7d258d0492c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 Marc-Antoine Ruel\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7d80c-f8b3-4b4e-abf2-4195bd2cb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circle_points(num_points=1000, radius=1.0, noise=0.1, seed=42):\n",
    "    \"\"\"Generate points on a circle in a deterministic way.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Generate angles.\n",
    "    theta = 2 * np.pi * np.random.rand(num_points)\n",
    "    # Convert to cartesian coordinates.\n",
    "    x = radius * np.cos(theta)\n",
    "    y = radius * np.sin(theta)\n",
    "    # Add some noise.\n",
    "    if noise > 0:\n",
    "        x += noise * np.random.randn(num_points)\n",
    "        y += noise * np.random.randn(num_points)\n",
    "    # Stack coordinates as pytorch tensors.\n",
    "    return torch.FloatTensor(np.column_stack((x, y)))\n",
    "\n",
    "\n",
    "def run_training(model, num_epochs=100):\n",
    "    train_points = generate_circle_points(num_points=1000)\n",
    "    test_points = generate_circle_points(num_points=200)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop.\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(train_points)\n",
    "        loss = criterion(outputs, train_points)\n",
    "        # Backward and optimize.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    # Evaluate the model.\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_outputs = model(test_points)\n",
    "        test_loss = criterion(test_outputs, test_points)\n",
    "        print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    return test_points, test_outputs, train_losses\n",
    "\n",
    "\n",
    "def draw_results(test_points, test_outputs, train_losses, model):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    # Plot training loss.\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # Plot original points and predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(test_points[:, 0].numpy(), test_points[:, 1].numpy(), label='Original Points', alpha=0.5)\n",
    "    plt.scatter(test_outputs[:, 0].numpy(), test_outputs[:, 1].numpy(), label='Predicted Points', alpha=0.5)\n",
    "    plt.title('Circle Points: Original vs Predicted')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate points in a grid to visualize the learned mapping\n",
    "    x = np.linspace(-2, 2, 20)\n",
    "    y = np.linspace(-2, 2, 20)\n",
    "    grid_x, grid_y = np.meshgrid(x, y)\n",
    "    grid_points = np.column_stack((grid_x.flatten(), grid_y.flatten()))\n",
    "\n",
    "    # Get predictions for grid points\n",
    "    with torch.no_grad():\n",
    "        grid_outputs = model(torch.FloatTensor(grid_points))\n",
    "    \n",
    "    # Plot the mapping from input to output\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(grid_points)):\n",
    "        plt.arrow(\n",
    "            grid_points[i, 0], grid_points[i, 1],\n",
    "            grid_outputs[i, 0].numpy() - grid_points[i, 0],\n",
    "            grid_outputs[i, 1].numpy() - grid_points[i, 1],\n",
    "            head_width=0.05, head_length=0.1, fc='blue', ec='blue', alpha=0.3\n",
    "        )\n",
    "    \n",
    "    plt.scatter(test_points[:, 0].numpy(), test_points[:, 1].numpy(), color='red', alpha=0.5)\n",
    "    plt.title('Vector Field of Learned Mapping')\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554d037-da48-4cf2-8234-2f153b3550ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/YihongDong/FANformer/blob/main/olmo/model.py#L77-L137\n",
    "# License unclear. That said, code is trivial.\n",
    "\n",
    "class FANLayer(nn.Module):\n",
    "    \"\"\"FANLayer: The layer used in FAN (https://arxiv.org/abs/2410.02675).\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): The number of input features.\n",
    "        output_dim (int): The number of output features.\n",
    "        p_ratio (float): The ratio of output dimensions used for cosine and sine parts (default: 0.25).\n",
    "        activation (str or callable): The activation function to apply to the g component. If a string is passed,\n",
    "            the corresponding activation from torch.nn.functional is used (default: 'gelu', we set to None in FANformer).\n",
    "        use_p_bias (bool): If True, include bias in the linear transformations of p component (default: True). \n",
    "            There is almost no difference between bias and non-bias in our experiments.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, p_ratio=0.25, activation=None, use_p_bias=True):\n",
    "        super(FANLayer, self).__init__()\n",
    "        assert 0 <= p_ratio <= 0.5, \"p_ratio must be between 0 and 0.5\"\n",
    "        self.p_ratio = p_ratio\n",
    "        p_output_dim = int(output_dim * self.p_ratio)\n",
    "        g_output_dim = output_dim - p_output_dim * 2  # Account for cosine and sine terms\n",
    "        self.input_linear = nn.Linear(input_dim, p_output_dim+g_output_dim, bias=use_p_bias)\n",
    "        self.fused_dims = (p_output_dim, g_output_dim)\n",
    "        self.activation = lambda x: x\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p, g = self.input_linear(x).split(self.fused_dims, dim=-1)\n",
    "        # Concatenate cos(p), sin(p), and activated g along the last dimension.\n",
    "        return torch.cat((torch.cos(p), torch.sin(p), self.activation(g)), dim=-1)\n",
    "\n",
    "class FAN(nn.Module):\n",
    "    \"\"\"Neural network with one hideen layer, using FAN.\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_size=64, output_dim=2, p_ratio=0.25, activation=None):\n",
    "        super(FAN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            FANLayer(input_dim, input_dim, p_ratio, activation),\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            FANLayer(hidden_size, hidden_size, p_ratio, activation),\n",
    "            nn.Linear(hidden_size, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242eb2c4-c58a-4e2b-84fc-e4ac826008be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    \"\"\"Neural network with one hidden layer.\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_size=64, output_dim=2):\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2047de94-a775-4a63-adf2-f764609d58e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SimpleNetwork(hidden_size=6)\n",
    "test_points, test_outputs, train_losses = run_training(model=model1)\n",
    "draw_results(test_points, test_outputs, train_losses, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a369218-fd77-4248-96e9-aac7866a48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FAN(hidden_size=6)\n",
    "test_points, test_outputs, train_losses = run_training(model=model2)\n",
    "draw_results(test_points, test_outputs, train_losses, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0860759-b2a5-4cff-b81e-c12e2e460a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
