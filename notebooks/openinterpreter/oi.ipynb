{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124134f6-67b1-402d-903c-e1b5aa5ca10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = \"Meta-Llama-3-8B-Instruct.Q5_K_M.llamafile\"\n",
    "print(f\"Setting up {llama}\")\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import urllib.request\n",
    "\n",
    "CACHE = os.path.join(os.getcwd(), \"..\", \"..\", \"cache\", \"openinterpreter\")\n",
    "LLAMAFILE = os.path.join(CACHE, llama)\n",
    "\n",
    "def progress_hook(block_num, block_size, total_size):\n",
    "    downloaded = block_num * block_size\n",
    "    percent = downloaded / total_size * 100\n",
    "    bar_length = 40\n",
    "    filled_length = int(bar_length * downloaded // total_size)\n",
    "    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n",
    "    sys.stdout.write(f'\\r|{bar}| {percent:.2f}%')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "if not os.path.isdir(CACHE):\n",
    "    os.mkdir(CACHE)\n",
    "if not os.path.isfile(LLAMAFILE):\n",
    "    sys.stdout.write(\"Downloading llama ... \")\n",
    "    url = f\"https://huggingface.co/Mozilla/Meta-Llama-3-8B-Instruct-llamafile/resolve/main/{llama}?download=true\"\n",
    "    urllib.request.urlretrieve(url, LLAMAFILE, reporthook=progress_hook)\n",
    "    print(\" ... Done!\")\n",
    "if not os.access(LLAMAFILE, os.X_OK):\n",
    "    os.chmod(LLAMAFILE, 0o770)\n",
    "\n",
    "def read_output(p):\n",
    "    while True:\n",
    "        output = p.stdout.readline()\n",
    "        if output:\n",
    "            print(output.decode().strip())\n",
    "        elif p.poll() is not None:\n",
    "            break\n",
    "\n",
    "port = \"8080\"\n",
    "llmserver = subprocess.Popen(\n",
    "    [LLAMAFILE, \"--nobrowser\", \"-ngl\", \"9999\", \"--port\", port],\n",
    "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n",
    "\n",
    "# Thread to handle the output asynchronously\n",
    "#thread = threading.Thread(target=read_output, args=(llmserver,), daemon=True)\n",
    "#thread.start()\n",
    "#thread.join()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0ef86-4ebb-4e7a-9e4a-bee1bfdddaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from interpreter import interpreter\n",
    "\n",
    "# https://docs.openinterpreter.com/settings/all-settings\n",
    "print(\"Configuring OI\")\n",
    "interpreter.offline = True\n",
    "# Tells OI to use an OpenAI-compatible server\n",
    "interpreter.llm.model = \"openai/local\"\n",
    "interpreter.llm.api_key = \"dummy_key\"\n",
    "interpreter.llm.api_base = f\"http://localhost:{port}/v1\"\n",
    "interpreter.llm.context_window = 7000\n",
    "interpreter.llm.max_tokens = 1000\n",
    "interpreter.llm.supports_functions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb243279-1ead-49e1-aea4-7a66b5761d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f899c69d-f24a-468d-af85-a6d9dedd8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmserver.kill()\n",
    "llmserver.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
